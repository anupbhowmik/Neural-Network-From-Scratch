{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    The base layer class for all layers\n",
    "\n",
    "    Attributes:\n",
    "        name: The name of the layer\n",
    "        input_shape: The shape of the input to the layer\n",
    "        output_shape: The shape of the output of the layer\n",
    "    \n",
    "    Methods:\n",
    "        forward: The forward pass of the layer\n",
    "        backward: The backward pass of the layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name):\n",
    "        \"\"\"\n",
    "        The constructor for the layer class\n",
    "\n",
    "        Parameters:\n",
    "            name: The name of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        self.name = name\n",
    "        self.input_shape = None\n",
    "        self.output_shape = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        The forward pass of the layer\n",
    "\n",
    "        Parameters:\n",
    "            x: The input to the layer\n",
    "\n",
    "        Returns:\n",
    "            The output of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, output_grad, learning_rate):\n",
    "        # we can use an optimizer instead of learning rate\n",
    "        \"\"\"\n",
    "        The backward pass of the layer\n",
    "\n",
    "        Parameters:\n",
    "            grad: The gradient of the loss w.r.t. the output of the layer\n",
    "\n",
    "        Returns:\n",
    "            The gradient of the loss w.r.t. the input of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_initialization(m, n):\n",
    "    \"\"\"\n",
    "    Xavier initialization for the weights of a layer\n",
    "\n",
    "    Parameters:\n",
    "        m: The number of rows in the weight matrix\n",
    "        n: The number of columns in the weight matrix\n",
    "\n",
    "    Returns:\n",
    "        The initialized weights\n",
    "    \"\"\"\n",
    "\n",
    "    random.seed(82)\n",
    "    \n",
    "    std_dev = np.sqrt(2.0 / (m + n))\n",
    "    return np.random.normal(0, std_dev, size=(m, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(Layer):\n",
    "    \"\"\"\n",
    "    The dense layer class\n",
    "\n",
    "    Attributes:\n",
    "        name: The name of the layer\n",
    "        input_shape: The shape of the input to the layer\n",
    "        output_shape: The shape of the output of the layer\n",
    "        weights: The weights of the layer\n",
    "        bias: The bias of the layer\n",
    "    \n",
    "    Methods:\n",
    "        forward: The forward pass of the layer\n",
    "        backward: The backward pass of the layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, input_shape, output_shape):\n",
    "        \"\"\"\n",
    "        The constructor for the dense layer class\n",
    "\n",
    "        Parameters:\n",
    "            name: The name of the layer\n",
    "            input_shape: The shape of the input to the layer\n",
    "            output_shape: The shape of the output of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(name)\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "     \n",
    "\n",
    "        # Xavier initialization\n",
    "        \n",
    "        self.weights = xavier_initialization(self.output_shape, self.input_shape)\n",
    "        \n",
    "        self.bias = np.zeros((self.output_shape, 1))\n",
    "\n",
    "        # print (\"in Xaviar init: \")\n",
    "        # print (\"weights: \\n\", self.weights)\n",
    "        # print (\"bias: \\n\", self.bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        The forward pass of the layer\n",
    "\n",
    "        Parameters:\n",
    "            input: The input to the layer\n",
    "\n",
    "        Returns:\n",
    "            The output of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        self.input = input\n",
    "\n",
    "        # print (\"input shape: \", self.input.shape)\n",
    "        # print (\"weights shape: \", self.weights.shape)\n",
    "        # print (\"bias shape: \", self.bias.shape)\n",
    "\n",
    "        return np.dot(self.weights, self.input) + self.bias\n",
    "    \n",
    "\n",
    "    def backward(self, output_grad, learning_rate):\n",
    "        \"\"\"\n",
    "        The backward pass of the layer\n",
    "\n",
    "        Parameters:\n",
    "            output_grad: The gradient of the loss w.r.t. the output of the layer\n",
    "     \n",
    "        Returns:\n",
    "            The gradient of the loss w.r.t. the input of the layer\n",
    "        \"\"\"\n",
    "        # output_grad: dL/dy (L: loss, y: output of the layer)\n",
    "        # output_grad: The gradient of the loss w.r.t. the output of the layer\n",
    "\n",
    "        weights_grad = np.dot(output_grad, self.input.T)\n",
    "        # dL/dB = dL/dy * dy/dB = dL/dy * 1\n",
    "        bias_grad = output_grad\n",
    "\n",
    "        # input_grad: dL/dx\n",
    "        input_grad = np.dot(self.weights.T, output_grad)\n",
    "\n",
    "    \n",
    "        self.weights -= learning_rate * weights_grad\n",
    "        self.bias -= learning_rate * bias_grad\n",
    "\n",
    "        return input_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationLayer(Layer):\n",
    "    \"\"\"\n",
    "    The activation layer class\n",
    "\n",
    "    Attributes:\n",
    "        name: The name of the layer\n",
    "        input_shape: The shape of the input to the layer\n",
    "        output_shape: The shape of the output of the layer\n",
    "    \n",
    "    Methods:\n",
    "        forward: The forward pass of the layer\n",
    "        backward: The backward pass of the layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, activation_func, activation_func_prime):\n",
    "        \"\"\"\n",
    "        The constructor for the activation layer class\n",
    "\n",
    "        Parameters:\n",
    "            name: The name of the layer\n",
    "            activation_func: The activation function of the layer\n",
    "            activation_func_prime: The derivative of the activation function of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(name)\n",
    "\n",
    "    \n",
    "        self.activation_func = activation_func\n",
    "        self.activation_func_prime = activation_func_prime\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        The forward pass of the layer\n",
    "\n",
    "        Parameters:\n",
    "            input: The input to the layer\n",
    "\n",
    "        Returns:\n",
    "            The output of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        self.input = input\n",
    "\n",
    "        return self.activation_func(input)\n",
    "\n",
    "    def backward(self, output_grad, learning_rate):\n",
    "        \"\"\"\n",
    "        The backward pass of the layer\n",
    "\n",
    "        Parameters:\n",
    "            output_grad: The gradient of the loss w.r.t. the output of the layer\n",
    "            learning_rate: dummy for activation layer\n",
    "\n",
    "        Returns:\n",
    "            The gradient of the loss w.r.t. the input of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        return np.multiply(output_grad, self.activation_func_prime(self.input))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxLayer(Layer):\n",
    "    \"\"\"\n",
    "    The softmax layer class\n",
    "\n",
    "    Attributes:\n",
    "        name: The name of the layer\n",
    "        input_shape: The shape of the input to the layer\n",
    "        output_shape: The shape of the output of the layer\n",
    "    \n",
    "    Methods:\n",
    "        forward: The forward pass of the layer\n",
    "        backward: The backward pass of the layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name):\n",
    "        \"\"\"\n",
    "        The constructor for the softmax layer class\n",
    "\n",
    "        Parameters:\n",
    "            name: The name of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(name)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        The forward pass of the layer\n",
    "\n",
    "        Parameters:\n",
    "            input: The input to the layer\n",
    "\n",
    "        Returns:\n",
    "            The output of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        z = input - max(input)\n",
    "        numerator = np.exp(z)\n",
    "        denominator = np.sum(numerator)\n",
    "        self.output =  numerator / denominator\n",
    "        return self.output\n",
    "\n",
    "\n",
    "    def backward(self, output_grad, learning_rate):\n",
    "        \"\"\"\n",
    "        The backward pass of the layer\n",
    "\n",
    "        Parameters:\n",
    "            output_grad: The gradient of the loss w.r.t. the output of the layer\n",
    "            learning_rate: dummy for softmax layer\n",
    "\n",
    "        Returns:\n",
    "            The gradient of the loss w.r.t. the input of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        n = np.size(self.output)\n",
    "        \n",
    "        tmp = np.tile(self.output, n)\n",
    "        # tile: construct an array by repeating A the number of times given by reps\n",
    "        \n",
    "\n",
    "        return np.dot(tmp * (np.identity(n) - np.transpose(tmp)), output_grad)\n",
    "        # * is element-wise multiplication, not matrix multiplication\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutLayer(Layer):\n",
    "    \"\"\"\n",
    "    The dropout layer class\n",
    "\n",
    "    Attributes:\n",
    "        name: The name of the layer\n",
    "        input_shape: The shape of the input to the layer\n",
    "        output_shape: The shape of the output of the layer\n",
    "        dropout_rate: The dropout rate of the layer\n",
    "    \n",
    "    Methods:\n",
    "        forward: The forward pass of the layer\n",
    "        backward: The backward pass of the layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, dropout_rate):\n",
    "        \"\"\"\n",
    "        The constructor for the dropout layer class\n",
    "\n",
    "        Parameters:\n",
    "            name: The name of the layer\n",
    "            dropout_rate: The dropout rate of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(name)\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.dropout_mask = None\n",
    "  \n",
    "\n",
    "    def forward(self, input, training = True):\n",
    "        \"\"\"\n",
    "        The forward pass of the layer\n",
    "\n",
    "        Parameters:\n",
    "            input: The input to the layer\n",
    "\n",
    "        Returns:\n",
    "            The output of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        if not training:\n",
    "            return input\n",
    "\n",
    "        self.input = input\n",
    "\n",
    "        np.random.seed(82)\n",
    "        D1 = np.random.rand(*input.shape) < (1 - self.dropout_rate)\n",
    "        # D1 is a binary mask\n",
    "        # print (\"D1: \\n\", D1)\n",
    "\n",
    "        self.dropout_mask = D1 / (1 - self.dropout_rate)\n",
    "\n",
    "        # print (\"dropout mask: \\n\", self.dropout_mask)\n",
    "\n",
    "        return np.multiply(input, self.dropout_mask) / (1 - self.dropout_rate)\n",
    "        # h/(1-p)\n",
    "\n",
    "    def backward(self, output_grad, learning_rate):\n",
    "        \"\"\"\n",
    "        The backward pass of the layer\n",
    "\n",
    "        Parameters:\n",
    "            output_grad: The gradient of the loss w.r.t. the output of the layer\n",
    "            learning_rate: dummy for dropout layer\n",
    "\n",
    "        Returns:\n",
    "            The gradient of the loss w.r.t. the input of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        return np.multiply(output_grad, self.dropout_mask) / (1 - self.dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLUActivationLayer(ActivationLayer):\n",
    "    \"\"\"\n",
    "    The ReLU activation layer class\n",
    "\n",
    "    Attributes:\n",
    "        name: The name of the layer\n",
    "        input_shape: The shape of the input to the layer\n",
    "        output_shape: The shape of the output of the layer\n",
    "    \n",
    "    Methods:\n",
    "        forward: The forward pass of the layer\n",
    "        backward: The backward pass of the layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name):\n",
    "        \"\"\"\n",
    "        The constructor for the ReLU activation layer class\n",
    "\n",
    "        Parameters:\n",
    "            name: The name of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(name, lambda x: np.maximum(x, 0), lambda x: np.where(x > 0, 1, 0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y, y_pred):\n",
    "    \"\"\"\n",
    "    The cross entropy loss function\n",
    "\n",
    "    Parameters:\n",
    "        y: The ground truth\n",
    "        y_pred: The predictions\n",
    "\n",
    "    Returns:\n",
    "        The loss\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    epsilon = 1e-15  # small value to avoid log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    # limits the values in the y_pred array to be within the range \n",
    "    # [epsilon, 1 - epsilon]. This ensures that the predicted probabilities are not exactly zero or one\n",
    "    \n",
    "    # print (\"in cross entropy loss\")\n",
    "    # print (\"y size: \", y.size)\n",
    "    # print (\"y_pred: \", y_pred)\n",
    "\n",
    "    return -np.sum(np.multiply(y, np.log(y_pred))) / y.size\n",
    "\n",
    "\n",
    "\n",
    "def cross_entropy_loss_prime(y, y_pred):\n",
    "    \"\"\"\n",
    "    The derivative of the cross entropy loss function with respect to the y_pred\n",
    "\n",
    "    Parameters:\n",
    "        y: The ground truth\n",
    "        y_pred: The predictions\n",
    "\n",
    "    Returns:\n",
    "        The derivative of the loss w.r.t. the predictions\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "\n",
    "    # print (\"in cross entropy loss prime\")\n",
    "    # print (\"y size: \", y.size)\n",
    "    # print (\"y_pred: \", y_pred)\n",
    "    \n",
    "    return -np.divide(y, y_pred) / y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, learning_rate=5e-3, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None  # First moment estimate\n",
    "        self.v = None  # Second moment estimate\n",
    "        self.t = 0  # Time step\n",
    "\n",
    "    def update(self, gradients):\n",
    "        if self.m is None:\n",
    "            self.m = np.zeros_like(gradients)\n",
    "            self.v = np.zeros_like(gradients)\n",
    "\n",
    "        self.t += 1\n",
    "\n",
    "        # Update biased first moment estimate\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * gradients\n",
    "\n",
    "        # Update biased second moment estimate\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * (gradients ** 2)\n",
    "\n",
    "        # Correct the bias in the moment estimates\n",
    "        m_hat = self.m / (1 - self.beta1 ** self.t)\n",
    "        v_hat = self.v / (1 - self.beta2 ** self.t)\n",
    "\n",
    "        # Update the parameters\n",
    "        update = self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "        \n",
    "        return update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moodle instructed dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as ds\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "train_validation_dataset = ds.EMNIST(root='./data', split='letters',\n",
    "                              train=True,\n",
    "                              transform=transforms.ToTensor(),\n",
    "                              download=True)\n",
    "\n",
    "\n",
    "independent_test_dataset = ds.EMNIST(root='./data', split='letters',\n",
    "                             train=False,\n",
    "                             transform=transforms.ToTensor())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset EMNIST\n",
       "    Number of datapoints: 124800\n",
       "    Root location: ./data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset EMNIST\n",
       "    Number of datapoints: 20800\n",
       "    Root location: ./data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "independent_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train-validation dataset as 85%-15% to form your train set and validation set using sklearn.model_selection.train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_dataset, validation_dataset = train_test_split(train_validation_dataset, test_size=0.15, random_state=82)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(dataset, num_images=10):\n",
    "    \"\"\"\n",
    "    Show the first num_images images of the dataset\n",
    "\n",
    "    Parameters:\n",
    "        dataset: The dataset to show the images from\n",
    "        num_images: The number of images to show\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a figure to display the images\n",
    "    fig = plt.figure()\n",
    "\n",
    "    # Loop over the first num_images images in the dataset\n",
    "    for i in range(num_images):\n",
    "        # Get the image and its label\n",
    "        image, label = dataset[i]\n",
    "\n",
    "        # The image needs to be transposed to be displayed correctly\n",
    "        image = image.transpose(0,2).transpose(0,1)\n",
    "\n",
    "        # Display the image\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow(image.squeeze(), cmap='gray')\n",
    "        plt.title(f'Label: {label}')\n",
    "        plt.axis('off')\n",
    "\n",
    "    # Display the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_images(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(x, y):\n",
    "\n",
    "    # normalize x\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    x = scaler.fit_transform(x.reshape(x.shape[0], 28 * 28))\n",
    "    x = x.reshape(x.shape[0], 28 * 28, 1)\n",
    "\n",
    "    # convert class labels to one-hot encoded, should have shape (?, 26, 1)\n",
    "    # 26 classes: 26 letters, for example, C is 3rd letter, then C is represented as [0, 0, 0, 1, 0, ...]\n",
    "    # one hot encoding\n",
    "    y = np.eye(27)[y]\n",
    "    # print (\"y\\n\", y)\n",
    "\n",
    "    print (\"y shape: \", y.shape)\n",
    "    \n",
    "    y = y.reshape(y.shape[0], 27, 1)\n",
    "\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape:  (106080, 27)\n",
      "y shape:  (18720, 27)\n",
      "y shape:  (20800, 27)\n"
     ]
    }
   ],
   "source": [
    "X = np.array([np.array(x[0]).flatten() for x in train_dataset])\n",
    "Y = np.array([x[1] for x in train_dataset])\n",
    "\n",
    "X_validation = np.array([np.array(x[0]).flatten() for x in validation_dataset])\n",
    "Y_validation = np.array([x[1] for x in validation_dataset])\n",
    "\n",
    "X_test = np.array([np.array(x[0]).flatten() for x in independent_test_dataset])\n",
    "Y_test = np.array([x[1] for x in independent_test_dataset])\n",
    "\n",
    "\n",
    "X, Y = preprocess_data(X, Y)\n",
    "X_validation, Y_validation = preprocess_data(X_validation, Y_validation)\n",
    "X_test, Y_test = preprocess_data(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 50/104 [01:03<01:20,  1.50s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "# XOR dataset\n",
    "\n",
    "# X = np.reshape([[0, 0], [0, 1], [1, 0], [1, 1]], (4, 2, 1))\n",
    "# # print (\"X: \", X)\n",
    "# Y = np.reshape([[0], [1], [1], [0]], (4, 1, 1))\n",
    "\n",
    "network = [\n",
    "    DenseLayer(\"Dense1\", X.shape[1], 72),\n",
    "    ReLUActivationLayer(\"ReLU1\"),\n",
    "    DropoutLayer(\"Dropout1\", 0.3),\n",
    "    DenseLayer(\"Dense2\", 72, 27),\n",
    "    SoftmaxLayer(\"Softmax\")\n",
    "]\n",
    "\n",
    "\n",
    "learning_rate = 5e-3\n",
    "epochs = 50\n",
    "\n",
    "adam_optimizer = AdamOptimizer(learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "\n",
    "    # use minibatch gradient descent\n",
    "    batch_size = 1024\n",
    "    num_batches = int(np.ceil(X.shape[0] / batch_size))\n",
    "    # print (\"num batches: \", num_batches)\n",
    "\n",
    "    random.seed(82)\n",
    "    # zip the data and shuffle\n",
    "    zipped_data = list(zip(X, Y))\n",
    "\n",
    "    random.shuffle(zipped_data)\n",
    "\n",
    "    for i in tqdm(range(num_batches)):\n",
    "        # print (\"batch: \", i)\n",
    "        \n",
    "        # get the minibatch\n",
    "        batching_time = time.time()\n",
    "        batch = zipped_data[i * batch_size : (i + 1) * batch_size]\n",
    "\n",
    "        # unzip the minibatch\n",
    "        X_batch, Y_batch = zip(*batch)\n",
    "\n",
    "        batching_time = time.time() - batching_time\n",
    "        # print (\"batching time: \", batching_time)\n",
    "\n",
    "        loop_time = time.time()\n",
    "        for x, y in zip(X_batch, Y_batch):\n",
    "            output = x\n",
    "            # print (\"output: \", output)\n",
    "\n",
    "            forward_time_start  = time.time()\n",
    "            for layer in network:\n",
    "                # print (\"layer: \", layer.name)\n",
    "                output = layer.forward(output)\n",
    "                # print (\"output: \\n\", output)\n",
    "\n",
    "\n",
    "            forward_time = time.time() - forward_time_start\n",
    "\n",
    "            # print(\"forward time: \", forward_time)\n",
    "\n",
    "            # loss += mse_loss(y, output)\n",
    "            # output_grad = mse_loss_prime(y, output)\n",
    "\n",
    "            loss_grad_time_start = time.time()\n",
    "\n",
    "            loss += cross_entropy_loss(y, output)\n",
    "            output_grad = cross_entropy_loss_prime(y, output)\n",
    "\n",
    "\n",
    "            # print (\"loss: \\n\", loss)\n",
    "            # print (\"output grad: \\n\", output_grad)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            loss_grad_time = time.time() - loss_grad_time_start\n",
    "            # print(\"loss grad time: \", loss_grad_time)\n",
    "\n",
    "\n",
    "            backward_time_start = time.time()\n",
    "            for layer in reversed(network):\n",
    "                output_grad = layer.backward(output_grad, learning_rate)\n",
    "                # print (\"layer: \", layer.name)\n",
    "                # print (\"output grad: \\n\", output_grad)\n",
    "\n",
    "                # ADAM\n",
    "                # if isinstance(layer, DenseLayer):  # Check if the layer is DenseLayer\n",
    "                #     # Access and update the parameters using AdamOptimizer\n",
    "                #     layer.weights -= adam_optimizer.update(layer.weights_gradient)\n",
    "                #     layer.biases -= adam_optimizer.update(layer.biases_gradient)\n",
    "\n",
    "            backward_time = time.time() - backward_time_start\n",
    "            # print(\"backward time: \", backward_time)\n",
    "\n",
    "     \n",
    "        loop_time = time.time() - loop_time\n",
    "        # print (\"loop time: \", loop_time)\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}: Loss = {loss}')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(network, X_test, Y_test):\n",
    "\n",
    "# test the model\n",
    "\n",
    "# print (\"X_test: \", X_test)\n",
    "# print (\"X_test shape:\", X_test.shape)\n",
    "\n",
    "# print (\"Y_test: \", Y_test)\n",
    "# print (\"Y_test shape:\", Y_test.shape)\n",
    "\n",
    "\n",
    "    output_list = []\n",
    "    for x in X_test:\n",
    "        output = x\n",
    "        for layer in network:\n",
    "            # if isinstance(layer, DropoutLayer):\n",
    "                # output = layer.forward(output, training=False)\n",
    "                \n",
    "            # else:\n",
    "            output = layer.forward(output)\n",
    "\n",
    "        output_list.append(output)\n",
    "\n",
    "        \n",
    "    # print (\"output: \", output_list)\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "    y_true = np.argmax(Y_test, axis=1)\n",
    "    y_pred = np.argmax(output_list, axis=1)\n",
    "\n",
    "    # print (\"y_true: \", y_true)\n",
    "    # print (\"y_pred: \", y_pred)\n",
    "\n",
    "\n",
    "    print (\"accuracy: \", accuracy_score(y_true, y_pred))\n",
    "    print (\"precision: \", precision_score(y_true, y_pred, average='macro'))\n",
    "    print (\"recall: \", recall_score(y_true, y_pred, average='macro'))\n",
    "    print (\"f1 score: \", f1_score(y_true, y_pred, average='macro'))\n",
    "\n",
    "    # print (\"confusion matrix: \")\n",
    "    # print (confusion_matrix(y_true, y_pred))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.7669777526395174\n",
      "precision:  0.7801897216207745\n",
      "recall:  0.7669105210921807\n",
      "f1 score:  0.7673200592006401\n"
     ]
    }
   ],
   "source": [
    "test_model(network, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation dataset for model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.7411324786324787\n",
      "precision:  0.7553494770742137\n",
      "recall:  0.7415243521355392\n",
      "f1 score:  0.7411517640602464\n"
     ]
    }
   ],
   "source": [
    "test_model(network, X_validation, Y_validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Independent test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true:  [[ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " ...\n",
      " [26]\n",
      " [26]\n",
      " [26]]\n",
      "y_pred:  [[ 1]\n",
      " [17]\n",
      " [ 1]\n",
      " ...\n",
      " [26]\n",
      " [26]\n",
      " [26]]\n",
      "accuracy:  0.7474519230769231\n",
      "precision:  0.7589957384036651\n",
      "recall:  0.7474519230769231\n",
      "f1 score:  0.747020009026171\n"
     ]
    }
   ],
   "source": [
    "test_model(network, X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
