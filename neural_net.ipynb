{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    The base layer class for all layers\n",
    "\n",
    "    Attributes:\n",
    "        name: The name of the layer\n",
    "        input_shape: The shape of the input to the layer\n",
    "        output_shape: The shape of the output of the layer\n",
    "    \n",
    "    Methods:\n",
    "        forward: The forward pass of the layer\n",
    "        backward: The backward pass of the layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name):\n",
    "        \"\"\"\n",
    "        The constructor for the layer class\n",
    "\n",
    "        Parameters:\n",
    "            name: The name of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        self.name = name\n",
    "        self.input_shape = None\n",
    "        self.output_shape = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        The forward pass of the layer\n",
    "\n",
    "        Parameters:\n",
    "            x: The input to the layer\n",
    "\n",
    "        Returns:\n",
    "            The output of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, output_grad, learning_rate):\n",
    "        # we can use an optimizer instead of learning rate\n",
    "        \"\"\"\n",
    "        The backward pass of the layer\n",
    "\n",
    "        Parameters:\n",
    "            grad: The gradient of the loss w.r.t. the output of the layer\n",
    "\n",
    "        Returns:\n",
    "            The gradient of the loss w.r.t. the input of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_initialization(m, n):\n",
    "    \"\"\"\n",
    "    Xavier initialization for the weights of a layer\n",
    "\n",
    "    Parameters:\n",
    "        m: The number of rows in the weight matrix\n",
    "        n: The number of columns in the weight matrix\n",
    "\n",
    "    Returns:\n",
    "        The initialized weights\n",
    "    \"\"\"\n",
    "\n",
    "    random.seed(282)\n",
    "    \n",
    "    weights = [[0 for _ in range(n)] for _ in range(m)]\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            weights[i][j] = random.gauss(0, math.sqrt(2.0 / (m + n)))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(Layer):\n",
    "    \"\"\"\n",
    "    The dense layer class\n",
    "\n",
    "    Attributes:\n",
    "        name: The name of the layer\n",
    "        input_shape: The shape of the input to the layer\n",
    "        output_shape: The shape of the output of the layer\n",
    "        weights: The weights of the layer\n",
    "        bias: The bias of the layer\n",
    "    \n",
    "    Methods:\n",
    "        forward: The forward pass of the layer\n",
    "        backward: The backward pass of the layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, input_shape, output_shape):\n",
    "        \"\"\"\n",
    "        The constructor for the dense layer class\n",
    "\n",
    "        Parameters:\n",
    "            name: The name of the layer\n",
    "            input_shape: The shape of the input to the layer\n",
    "            output_shape: The shape of the output of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(name)\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "     \n",
    "        # self.weights = np.array(xavier_initialization(output_shape, input_shape))\n",
    "        # w_ji -> j: output, i: input\n",
    "\n",
    "        np.random.seed(182)\n",
    "        \n",
    "        self.weights = np.random.randn(output_shape, input_shape)\n",
    "        self.bias = np.random.randn(output_shape, 1)\n",
    "        # randn returns a sample from the \"standard normal\" distribution\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        The forward pass of the layer\n",
    "\n",
    "        Parameters:\n",
    "            input: The input to the layer\n",
    "\n",
    "        Returns:\n",
    "            The output of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        self.input = input\n",
    "\n",
    "        # print (\"input shape: \", self.input.shape)\n",
    "        # print (\"weights shape: \", self.weights.shape)\n",
    "        # print (\"bias shape: \", self.bias.shape)\n",
    "\n",
    "        return np.dot(self.weights, self.input) + self.bias\n",
    "    \n",
    "\n",
    "    def backward(self, output_grad, learning_rate):\n",
    "        \"\"\"\n",
    "        The backward pass of the layer\n",
    "\n",
    "        Parameters:\n",
    "            output_grad: The gradient of the loss w.r.t. the output of the layer\n",
    "     \n",
    "        Returns:\n",
    "            The gradient of the loss w.r.t. the input of the layer\n",
    "        \"\"\"\n",
    "        # output_grad: dL/dy (L: loss, y: output of the layer)\n",
    "        # output_grad: The gradient of the loss w.r.t. the output of the layer\n",
    "\n",
    "        # todo: do the math and keep note for evaluation\n",
    "    \n",
    "        weights_grad = np.dot(output_grad, self.input.T)\n",
    "        # dL/dB = dL/dy * dy/dB = dL/dy * 1\n",
    "        bias_grad = output_grad\n",
    "\n",
    "        # input_grad: dL/dx\n",
    "        input_grad = np.dot(self.weights.T, output_grad)\n",
    "\n",
    "        # todo minibatch gradient descent\n",
    "\n",
    "        self.weights -= learning_rate * weights_grad\n",
    "        self.bias -= learning_rate * bias_grad\n",
    "\n",
    "        return input_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationLayer(Layer):\n",
    "    \"\"\"\n",
    "    The activation layer class\n",
    "\n",
    "    Attributes:\n",
    "        name: The name of the layer\n",
    "        input_shape: The shape of the input to the layer\n",
    "        output_shape: The shape of the output of the layer\n",
    "    \n",
    "    Methods:\n",
    "        forward: The forward pass of the layer\n",
    "        backward: The backward pass of the layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, activation_func, activation_func_prime):\n",
    "        \"\"\"\n",
    "        The constructor for the activation layer class\n",
    "\n",
    "        Parameters:\n",
    "            name: The name of the layer\n",
    "            activation_func: The activation function of the layer\n",
    "            activation_func_prime: The derivative of the activation function of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(name)\n",
    "\n",
    "    \n",
    "        self.activation_func = activation_func\n",
    "        self.activation_func_prime = activation_func_prime\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        The forward pass of the layer\n",
    "\n",
    "        Parameters:\n",
    "            input: The input to the layer\n",
    "\n",
    "        Returns:\n",
    "            The output of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        self.input = input\n",
    "\n",
    "        return self.activation_func(input)\n",
    "\n",
    "    def backward(self, output_grad, learning_rate):\n",
    "        \"\"\"\n",
    "        The backward pass of the layer\n",
    "\n",
    "        Parameters:\n",
    "            output_grad: The gradient of the loss w.r.t. the output of the layer\n",
    "            learning_rate: dummy for activation layer\n",
    "\n",
    "        Returns:\n",
    "            The gradient of the loss w.r.t. the input of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        return np.multiply(output_grad, self.activation_func_prime(self.input))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLUActivationLayer(ActivationLayer):\n",
    "    \"\"\"\n",
    "    The ReLU activation layer class\n",
    "\n",
    "    Attributes:\n",
    "        name: The name of the layer\n",
    "        input_shape: The shape of the input to the layer\n",
    "        output_shape: The shape of the output of the layer\n",
    "    \n",
    "    Methods:\n",
    "        forward: The forward pass of the layer\n",
    "        backward: The backward pass of the layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name):\n",
    "        \"\"\"\n",
    "        The constructor for the ReLU activation layer class\n",
    "\n",
    "        Parameters:\n",
    "            name: The name of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(name, lambda x: np.maximum(x, 0), lambda x: np.where(x > 0, 1, 0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y, y_pred):\n",
    "    \"\"\"\n",
    "    The cross entropy loss function\n",
    "\n",
    "    Parameters:\n",
    "        y: The ground truth\n",
    "        y_pred: The predictions\n",
    "\n",
    "    Returns:\n",
    "        The loss\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    epsilon = 1e-15  # small value to avoid log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    # limits the values in the y_pred array to be within the range \n",
    "    # [epsilon, 1 - epsilon]. This ensures that the predicted probabilities are not exactly zero or one\n",
    "    \n",
    "    # print (\"y size: \", y.size)\n",
    "    # print (\"y_pred: \", y_pred)\n",
    "\n",
    "    return -np.sum(np.multiply(y, np.log(y_pred))) / y.size\n",
    "\n",
    "    # return -np.sum(y * np.log(y_pred)) / y.size\n",
    "\n",
    "def cross_entropy_loss_prime(y, y_pred):\n",
    "    \"\"\"\n",
    "    The derivative of the cross entropy loss function\n",
    "\n",
    "    Parameters:\n",
    "        y: The ground truth\n",
    "        y_pred: The predictions\n",
    "\n",
    "    Returns:\n",
    "        The derivative of the loss w.r.t. the predictions\n",
    "    \"\"\"\n",
    "\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "\n",
    "    return -np.divide(y, y_pred) / y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y, y_pred):\n",
    "    \"\"\"\n",
    "    The mean squared error loss function\n",
    "\n",
    "    Parameters:\n",
    "        y: The ground truth\n",
    "        y_pred: The predictions\n",
    "\n",
    "    Returns:\n",
    "        The loss\n",
    "    \"\"\"\n",
    "\n",
    "    return np.mean(np.square(y - y_pred))\n",
    "\n",
    "def mse_loss_prime(y, y_pred):\n",
    "    \"\"\"\n",
    "    The derivative of the mean squared error loss function\n",
    "\n",
    "    Parameters:\n",
    "        y: The ground truth\n",
    "        y_pred: The predictions\n",
    "\n",
    "    Returns:\n",
    "        The derivative of the loss w.r.t. the predictions\n",
    "    \"\"\"\n",
    "\n",
    "    return 2 * (y_pred - y) / y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moodle instructed dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as ds\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "train_validation_dataset = ds.EMNIST(root='./data', split='letters',\n",
    "                              train=True,\n",
    "                              transform=transforms.ToTensor(),\n",
    "                              download=True)\n",
    "\n",
    "\n",
    "# independent_test_dataset = datasets.dsets.EMNIST(root='./data',\n",
    "#                        split='letters',\n",
    "#                              train=False,\n",
    "#                              transform=transforms.ToTensor())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset EMNIST\n",
       "    Number of datapoints: 124800\n",
       "    Root location: ./data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train-validation dataset as 85%-15% to form your train set and validation set using sklearn.model_selection.train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_dataset, validation_dataset = train_test_split(train_validation_dataset, test_size=0.15, random_state=82)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(dataset, num_images=5):\n",
    "    \"\"\"\n",
    "    Show the first num_images images of the dataset\n",
    "\n",
    "    Parameters:\n",
    "        dataset: The dataset to show the images from\n",
    "        num_images: The number of images to show\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Create a figure to display the images\n",
    "    fig = plt.figure()\n",
    "\n",
    "    # Loop over the first num_images images in the dataset\n",
    "    for i in range(num_images):\n",
    "        # Get the image and its label\n",
    "        image, label = dataset[i]\n",
    "\n",
    "        # The image needs to be transposed to be displayed correctly\n",
    "        image = image.transpose(0,2).transpose(0,1)\n",
    "\n",
    "        # Display the image\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow(image.squeeze(), cmap='gray')\n",
    "        plt.title(f'Label: {label}')\n",
    "        plt.axis('off')\n",
    "\n",
    "    # Display the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgQAAAB/CAYAAACQeNq9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAc5klEQVR4nO3deXRU5fkH8G/YkrBoAiFIAFkqGkAQaspWlihL2IphKY2i0FaKx0oLHBBtDYsVKBSwVgKIlUVMEGggFhWw7BUOi5R9hwBikB0DUiEBcn9/8Mvb753MhCyT2fh+zuGch1nv3Hfu8PI87xJkWZYFERERua+V8vYBiIiIiPepQyAiIiLqEIiIiIg6BCIiIgJ1CERERATqEIiIiAjUIRARERGoQyAiIiJQh0BERETgQx2CU6dOISgoCFOnTnXba27YsAFBQUHYsGGD215T7lJ7+S61jW9Su/gGtYNrxeoQzJ8/H0FBQdixY4e7jsenHDlyBMOHD0fr1q0REhKCoKAgnDp1Ks/jLl++jClTpqBdu3aoWrUqwsLC0LJlSyxevNjzB52PQG+vtLQ0xMXFISoqCsHBwahZsyb69u2L/fv3e/vQ7inQ26ZOnToICgpy+qd+/frePjyXAr1dAGDNmjV46qmnEBERgbCwMDRv3hwfffSRtw/LJtDbYdmyZfjFL36BevXqoXz58njssccwYsQIZGZmOn388uXL8eMf/xghISF4+OGHMXbsWNy+fbvYx1Gm2K8QwLZs2YJ3330XDRs2RIMGDbB7926Xj3vjjTfQrVs3JCYmokyZMli6dCkSEhJw8OBBvPnmm5498PvUvn37EB4ejqFDhyIiIgLnzp3D3Llz0bx5c2zZsgVPPPGEtw/xvvXOO+/g+vXrttu+/vprJCYmonPnzl46Klm+fDni4+PRqlUrjBs3DkFBQViyZAkGDBiAS5cuYfjw4d4+xPvC4MGDERUVheeffx4PP/ww9u3bh6SkJKxYsQI7d+5EaGioeezKlSsRHx+P2NhYTJ8+Hfv27cP48eNx4cIFzJo1q1jHoQ5BPnr27InMzExUqlQJU6dOddkhaNSoEY4dO4batWub237729+iY8eOmDx5MkaNGoUKFSp46KjvX2PGjMlz26BBg1CzZk3MmjUL7733nheOSgAgPj4+z23jx48HAPTv39/DRyO5kpKSUL16daxbtw7BwcEAgJdeegnR0dGYP3++OgQekpqaitjYWNttTz75JAYOHIiUlBQMGjTI3D5y5Eg0adIE//rXv1CmzN1/wh944AFMnDgRQ4cORXR0dJGPo8THEGRnZ2PMmDF48skn8eCDD6JChQpo27Yt1q9f7/I5f/3rX1G7dm2Ehoaiffv2TlO+hw8fRt++fVG5cmWEhIQgJiYGy5cvv+fx/PDDDzh8+DAuXbp0z8dWrlwZlSpVuufj6tata+sMAEBQUBDi4+ORlZWFEydO3PM1fIU/t5czkZGRKF++vMvUmz8JtLZZuHAh6tati9atWxfp+b7Cn9vl2rVrCA8PN50BAChTpgwiIiJs/yv1B/7cDo6dAQDo1asXAODQoUPmtoMHD+LgwYMYPHiw6QwAd/8DalkWUlNT7/le+SnxDsG1a9fwwQcfIDY2FpMnT8a4ceNw8eJFxMXFOf0f94IFC/Duu+/ilVdewR/+8Afs378fTz/9NM6fP28ec+DAAbRs2RKHDh3C66+/jmnTpqFChQqIj49HWlpavsezfft2NGjQAElJSe7+qHmcO3cOABAREVHi7+UugdBemZmZuHjxIvbt24dBgwbh2rVr6NChQ4Gf76sCoW1y7dq1C4cOHcJzzz1X6Of6Gn9ul9jYWBw4cACjR4/G8ePHkZ6ejrfeegs7duzAqFGjCn0uvMmf28EZZ/9+7Nq1CwAQExNje2xUVBRq1qxp7i8yqxjmzZtnAbC++uorl4+5ffu2lZWVZbvtu+++s6pVq2b9+te/NredPHnSAmCFhoZaGRkZ5vZt27ZZAKzhw4eb2zp06GA1btzYunnzprktJyfHat26tVW/fn1z2/r16y0A1vr16/PcNnbs2EJ91ilTplgArJMnTxbo8ZcvX7YiIyOttm3bFup9StL90l6PPfaYBcACYFWsWNFKTEy07ty5U+Dne8P90ja5RowYYQGwDh48WOjnelKgt8v169etfv36WUFBQeaaKV++vPXJJ5/c87meFOjt4MyLL75olS5d2jp69Ki5LfffodOnT+d5/E9+8hOrZcuWRXqvXCWeIShdujTKlSsHAMjJycGVK1dw+/ZtxMTEYOfOnXkeHx8fjxo1api/N2/eHC1atMCKFSsAAFeuXMG6devQr18/fP/997h06RIuXbqEy5cvIy4uDseOHcOZM2dcHk9sbCwsy8K4cePc+0FJTk4O+vfvj8zMTEyfPr3E3qckBEJ7zZs3D6tWrcLMmTPRoEED3LhxA3fu3Cnw831VILRN7rEvWrQIzZo1Q4MGDQr1XF/kz+0SHByMRx99FH379sXHH3+M5ORkxMTE4Pnnn8fWrVsLeSa8y5/bwdHChQsxZ84cjBgxwjYL58aNGwBgK/HkCgkJMfcXlUcGFX744YeYNm0aDh8+jFu3bpnb69atm+exzqYgPfroo1iyZAkA4Pjx47AsC6NHj8bo0aOdvt+FCxdsDe1pv/vd77Bq1SosWLDAL0e2+3t7tWrVysQJCQnmHx13zjv2Fn9vGwDYuHEjzpw5E1AD1vy1XYYMGYKtW7di586dKFXq7v8P+/Xrh0aNGmHo0KHYtm1bsd/Dk/y1HdiXX36JF198EXFxcZgwYYLtvtxxHVlZWXmed/PmzWKP+yjxDkFycjJ++ctfIj4+Hq+++ioiIyNRunRp/PnPf0Z6enqhXy8nJwfA3ZGWcXFxTh/zyCOPFOuYi+PNN9/EzJkzMWnSJLzwwgteO46iCrT2Cg8Px9NPP42UlBS/7xAEStukpKSgVKlSePbZZ93+2t7gr+2SnZ2NOXPmYNSoUaYzAABly5ZF165dkZSUhOzsbPO/bl/nr+3A9uzZg549e+Lxxx9HamqqbeAgAFSvXh0AcPbsWdSqVct239mzZ9G8efNivX+JdwhSU1NRr149LFu2DEFBQeb2sWPHOn38sWPH8tx29OhR1KlTBwBQr149AHe/tB07dnT/ARfDjBkzMG7cOAwbNgyvvfaatw+nSAKxvW7cuIGrV6965b3dKRDaJisrC0uXLkVsbCyioqI88p4lzV/b5fLly7h9+7bTctqtW7eQk5PjV6U2f22HXOnp6ejSpQsiIyOxYsUKVKxYMc9jmjZtCgDYsWOH7R//b7/9FhkZGRg8eHCxjsEjYwgAwLIsc9u2bduwZcsWp4//5JNPbHWZ7du3Y9u2bejatSuAu9PIYmNjMXv2bJw9ezbP8y9evJjv8RR3qpQrixcvxu9//3v0798fb7/9tltf25P8ub0uXLiQ57ZTp05h7dq1eUbl+iN/bptcK1asQGZmZkCtPeCv7RIZGYmwsDCkpaUhOzvb3H79+nV8+umniI6O9quph/7aDsDdGQWdO3dGqVKl8MUXX6Bq1apOH9eoUSNER0fj/ffft3XWZs2ahaCgIPTt2/ee75Uft2QI5s6di1WrVuW5fejQoejRoweWLVuGXr16oXv37jh58iTee+89NGzYMM/KZcDdFEybNm3w8ssvIysrC++88w6qVKlimwIzY8YMtGnTBo0bN8ZvfvMb1KtXD+fPn8eWLVuQkZGBPXv2uDzW7du346mnnsLYsWPvOdjj6tWrZlDg5s2bAdxdyCMsLAxhYWEYMmSIec0BAwagSpUq6NChA1JSUmyv07p1a9Pb9AWB2l6NGzdGhw4d0LRpU4SHh+PYsWOYM2cObt26hUmTJhX8BHlRoLZNrpSUFAQHB6NPnz4FeryvCMR2KV26NEaOHInExES0bNkSAwYMwJ07dzBnzhxkZGQgOTm5cCfJAwKxHQCgS5cuOHHiBEaNGoVNmzZh06ZN5r5q1aqhU6dO5u9TpkxBz5490blzZyQkJGD//v1ISkrCoEGDij9ItzhTFHKngrj6880331g5OTnWxIkTrdq1a1vBwcFWs2bNrM8++8waOHCgVbt2bfNauVNBpkyZYk2bNs2qVauWFRwcbLVt29bas2dPnvdOT0+3BgwYYD300ENW2bJlrRo1alg9evSwUlNTzWOKOxUk95ic/eFjv9d5mDdvXhHOrvsFenuNHTvWiomJscLDw60yZcpYUVFRVkJCgrV3797inDaPCPS2sSzLunr1qhUSEmL17t27qKfJ4+6HdklJSbGaN29uhYWFWaGhoVaLFi1s7+ELAr0d8vts7du3z/P4tLQ0q2nTplZwcLBVs2ZNKzEx0crOzi7MKXUq6P8PRkRERO5jPrP9sYiIiHiPOgQiIiKiDoGIiIioQyAiIiJQh0BERESgDoGIiIigEAsT8VKQ4j7umPWptikZxW0btUvJ0DXju3TN+KaCtosyBCIiIqIOgYiIiHhgt0MREZGSkLuhEWBPi+duXSyFowyBiIiIqEMgIiIiKhmIiIgfqVOnjomfeeYZE+/du9fEGzduNLHKBwWnDIGIiIioQyAiIiJAkFXAFQu0YETJ0CIrvkuLrPgmXTO+q6SuGZ5NsHnzZhPHxMSYeNeuXSb++c9/buJTp04V65gCgRYmEhERkQJTh0BEREQ0y8DdypT53ym9ffu2F4/E93EasHLlyiauUKGCifl8Mj63586ds91369YtE9+5c6fYxyki3sUp74yMDBNzyeCBBx4wcXBwsGcOLMAoQyAiIiLqEIiIiIhKBgXmOPq1R48eJm7Xrp2Ju3XrZuLk5GQTT5482cT380IZXCaoUqWKiX/605+amBce4TQgu3btmonXrl1ru49LCJcuXTLx/XzeRfwZX7sHDhwwcc+ePU0cFhZm4oYNG5r4yJEjJXtwAUQZAhEREVGHQERERFQyKLAmTZrY/r5w4UIT86h49tprr5l4wYIFJj5z5oybj873cImlatWqJm7btq2JExISnN5eqVIlE5ctW9bp6/NMgoEDB9ruO3r0qIlnz55t4g0bNphYsw/cJyIiwunt3333ne3vOufiDrt37zYxf8fCw8NNzL/XaWlpHjmuQKAMgYiIiKhDICIiIioZ5Cs0NNTE8+fPt93HZQJOj/MCGmvWrDGx4+I5gYI/e7ly5UwcHR1t4mHDhpm4a9euJuZZBqVK/a9vyufQ1cwAXnikcePGtvt4hDGXHE6ePGniEydOOH1dcY3bunbt2ibmGTSMy2oAsHr1ahP/8MMPbj46uV8cPHjQxJmZmSbmkoEUjTIEIiIiog6BiIiIqEMgIiIi0BiCPLiW/eqrr5rYsU7tatwAr4w3Y8YMEwfSlCtebbBu3bombtGihYlHjhxpYh5PwOMMsrKyTMxTBY8fP25iXpWMNW3a1MSdO3e23cfjCzp27GjiXr16mfhvf/ubiX19EypXm0ABwIMPPuj0OVevXjUxT80qzmfl9x49erSJ+/Tp4/TxvPEMYJ96++GHH5pY+9WXnMjISBP379/fxK5WAM0Pj7v57LPPTOw4vbSkBdJvqa9RhkBERETUIRARERGVDPLgqXCvvPKKibmUANjLBJyG5TLD+vXrS+IQvYJLJK42JeL0/COPPGJiTuG7KhPwamJcJti3b5/T4/nmm29MzKUKx+Pj937ooYdMzNMRfaVkUNjVHYG8paxc+/fvN/GWLVtMvHTpUhN//fXXJubvc0HwSpHp6ekm5vMaFRVlew6XkdiUKVNMrOmIBcelJJ4G+vLLL5v4hRdeMDGXD4oiOzvbxI8//riJPV0ykJKjDIGIiIioQyAiIiIqGQCwp2q7d+9uYk49O+JV7/70pz+Z2HF1Nn/F5wQA6tWrZ+LExEQT88qDPOKdU/WcavznP/9p4r/85S8mPnTokNPHuxpRzGlKPgYAaNOmjYk59R4bG2viatWqmdhXRrnzsU6YMMHEP/vZz0zsOMvAFZ79ERcXZ2JO9fLmWxcuXLjna165csXE/B3gfeg55scA9mtrwIABJuZZJVzSuHnz5j2PKZCUKVPGacwr8PE1xr877dq1M3FBSgNcugPsZTouMQ0aNMjEfI3yCoG+gstY165dK5H34HbxNE+UNpUhEBEREXUIRERERCUDAPZ0+BtvvGFix5kF7KOPPjLxP/7xDxNz2sqfOZZL+Lz069fPxLwBlCucnrxx44aJa9asaWJOWRZk4REuGezdu9d2X6NGjUzMaXhejMWbqT/G569bt24m7tmzp4n5uFetWmV7/q5du0zMG0Fx+z333HMm5vLK8uXLTcylHFcbSrlagItjHvk+b9482/N5RkSdOnVMPG7cOBNzqvfzzz+/5zH5EsfvFJfNOPW+bNkyE//qV78y8bPPPmtiXsDL1aY9XD5wtTkYX3t8jb311lu21+JzzeVCXsArIyPDxL5Yzjl//ryJv/zyS5eP43biz1qjRg2nj+HzzKWZoizuVFh8PWzcuNHE3JaO5Z/iUIZARERE1CEQERGR+6xkEBISYmIe8cyj3Xl0NnMchT1z5kwT+/NiKpwy4zQlj0YHgPbt25vYVZnA1f4OlSpVMjGvp96wYUMTc9qfU5OuRtZyWcGxZMCpaV4gyVfSzpxK7t27t4l5VD6nKb/66isT8x4CgH1veD7n/Lm7dOliYk7V83n69NNPTVyc88Ttsnr1att9XGYbNWqUiXlRnWbNmpl45cqVbjkmd+AU8sMPP2xiXviHZ4MA9hkh/JwRI0aYmNPUfO6Sk5NNzAtvbd682cTx8fEmjoiIMDGnkLlUsXjxYhMXdMQ6zwDxFbxPBy9cxrNp+Lri33rA3k7828QlTF5gi797/BvnOBOrJLhaAO+ZZ54x8dq1a50+viiUIRARERF1CERERCRASwauSgOcbuWR6Dwy2lXae9asWbb3KMhCLv6AFzFp2bKliXnkN2BPpzFXaTNXt3PqlRexKeyof07j8YIpgD2N3qNHDxPz2v3//e9/C/V+7sRp4mHDhpmYy1U8ivj11183MX82wPWsloKkDks65elYSuMtcwcOHGhiPh+eSMPmh2elcJmMF3HiEgen6h1dvHjRxDyzgEf0cwmHS1+uZlhwWYEXEPrggw9MfPr0aRMvWbLExL6yZ0dx8aJIfJ1wKWDw4MEmLl++vO35XLJzNSODz+Hu3budvp8nylg8e4SvE/6uFLdMYHs/t72SiIiI+C11CERERMT/SgZcDuBR6ryoR6dOnUzsqjTA6aHvv//exJzG5rQyj5D2dzyClkejx8TEmPhHP/qRy+f4mgoVKtj+7pgizHX9+nUTezp9yiURHiHM32H+TvL3bdu2bSbO77j5PTh96qrc485UozOO7cDlG95LwhNr0OeHZwPMmDHDxFwy4NIAL8pz7NgxEzse++TJk03Mi8rwnhBcIuEUdEHS0TxzgV+Hr1X+TgTKomn8vXU1A4AX5nJcRIn3oeFyAM/g4Pbirda5XFHS1w9g/0z8O8czLdxJGQIRERFRh0BERES8UDLgUZOO6+XzYiyc3udUIy8EwulWV6PUeWvbnTt3mjglJcXEY8aMMTGXDN5//30Tc5rJH3Hqic/bpEmTTFy/fn0TV6xYsUCvVRCuUmucYi1sCp/bm9cXB+ypXv6+cRtymcjTeDEUTu9yiYpHqBc0Nclr3vOMEV5wivcd4EVd3DVimssEvXr1st3H1y6P9N6/f7+JeZEVT5V1uGTA5RzGvx282I+rhaGAkj9+HmnO11KtWrVMzNeC4z4YgYxnY6Smptru4/0ZeOYR74/iizMyPLHltDIEIiIiog6BiIiIFLFkwGlOTuc3adLkns/l9dPbtm1ru8/VNp+uygE8evTEiRMm5lTakCFDTMzpUk4lc9qc8Zr6nhhRWpJ4MZWRI0eamGcWcBrXkbvKBLz9MS9Uc+7cORMXNl3nuA2pq21JvTnLgFPynKrnEeecum7atKmJ161b5/R1AHtpwdWeAIy3heVR1YUtGfB3hWfy8LbNXCJwPD5e2GvRokUmPnr0aKGOwx14FDmXEvl7xPtMpKenm9ibI/e5Lfn374knnjAxL8bG5VPHUeqcjubP5Iupc1f4WP/+97+bmPfNANy7XXCgUYZARERE1CEQERGRIpYMoqKiTJyUlGTi6tWrF+p1HNNRnGY+c+aMiTmFtWfPHhOPHz/exLyePY8wdZUK5ZIBjzjnY+LtV/0Rl1r69OljYt421VWZoChryrsqE3CK7j//+Y+Jly5damLHxUPcxduL3uTi7yGn6jnty+3iaptox/LZ9OnTTcwj5Hndcy6V8Ra4ly9fNjFfDxxzeZCvb57Zwel0fozjYlY8y2PChAkm5tJRSX0P8uNqu2Au2/Dn4nIHP9fT+2Nwu/IW17NnzzYxl3T5t5PLZ4C9hMBlLN4Wm69XnhniafzbxN9V/v3hz6ASQcEpQyAiIiLqEIiIiEgRSwbffvutiTk9xWlOVzhtu2LFCtt9nK7n+zidxTGPsC4sLitwGpvThA0aNDDx4cOHi/xe3sKzNlq1amXi0NBQp48vaJmgIDMueDYBb/06depUEx85cqRA73cvjiOmOaV7/vx5E69evdrE3hw9zal6HmHPs254rf8OHTqY2LEExilTHu3P6Xr+7vJsDt7HgssSPBOIFwuLjY01MS9+w7MjeKGXtLQ027Hy+vBr1qwxMX9XvI3PLy9G5Ov4O5+QkGBiLhnw7dHR0bbnc4mJ25Znq8TFxZm4a9euJi6pdfVd4TX9eT8HvhbyW1hNXFOGQERERNQhEBERkSKWDHjkNo/0nzhxYqFex5tpW06Pc2qd005cPnBMf/oDTq3xojD8GYsym8AVLsPwLBFeQ523iy3Ooi6c2uXXBOzbBfNIbE6XexOfJ55lsHz5chPzKH6eCcIzfAB7mcFVW/KCYXPnznV6HJz25wV5uDzEMZdiuBSQnJxsYi4LAPbSgLv2TpC8eFQ9zwzg3zAuCwD2a7FFixYmrlOnjol5bxdPz6hwxZ2/X6IMgYiIiEAdAhEREYEbtj/m1J8/pQH5WD/++GMT9+7d28QvvfSSiXmkPGBf5MNX8SI2hR11m99MAk4186hyXqN//vz5Jv7iiy9M7K6FZ7j9ePYAAGzatMnEnD71xqI398IzZf74xz+amLfh5vTu22+/bXs+zyxwhUtzPMuHR9Fzeez06dMmPnDggIl5vft///vfTh/PC8Lw90S8j68Zvm4d+Xp5lK9pV9uEOy68JAWjDIGIiIioQyAiIiJuKBkEAlfbiPLCLampqbbndOrUycS8ragv4QVDOJ3Pn6t06dJOn+tYMuDUHJ8vHlXO78EpyZJOHWdnZ+f7d1/G55lnRHDM2/OOGTPG9nxeOIZHXPPrcrtwqYvfg2cZ8AhyLhPwa/rTtrgSuDTLwL2UIRARERF1CEREREQdAhEREYHGEACw11J5046VK1ea2B83y+ApYFzr51ULeXobc9ywZMaMGSbm88Ib9WiaWcngaVaff/657T5uC1cKMjWYpyOK+DLeiKl79+4mLleunIl5pU9ebRHw3TFfvkAZAhEREVGHQERERFQyyCM9Pd3ErVu3NrFjycAf0k6cwt+7d6+JFy1aZGLeyIY5ppB5qqHKBN7jmPL3p9VBRdyNN2rjmEsGHTt2tD1n3rx5Jtbvl50yBCIiIqIOgYiIiKhkkC9XK8f5Iy5xTJ8+vdDP18p0IuJruGTGK2neuHHDxFu3brU9R2UC15QhEBEREXUIREREBAiy8tv4nh+oTSRKRAFPf77UNiWjuG2jdikZumZ8lyeumeDgYBPzJnO8adu6detMzJt7Ae75/vibgn5mZQhEREREHQIRERFRycDrlP70XSoZ+CZdM77L09cML0bEz9VMAjuVDERERKTA1CEQERERLUwkIiL+SXt5uJcyBCIiIqIOgYiIiBRiloGIiIgELmUIRERERB0CERERUYdAREREoA6BiIiIQB0CERERgToEIiIiAnUIREREBOoQiIiICNQhEBEREQD/B546TWWdB2X7AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_images(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tLoss: 0.8421013\n",
      "Epoch: 1\tLoss: 0.5714817\n",
      "Epoch: 2\tLoss: 0.3888343\n",
      "Epoch: 3\tLoss: 0.2844479\n",
      "Epoch: 4\tLoss: 0.1974869\n",
      "Epoch: 5\tLoss: 0.1227138\n",
      "Epoch: 6\tLoss: 0.0569513\n",
      "Epoch: 7\tLoss: 0.0000000\n",
      "Epoch: 8\tLoss: 0.0000000\n",
      "Epoch: 9\tLoss: 0.0000000\n",
      "Epoch: 10\tLoss: 0.0000000\n",
      "Epoch: 11\tLoss: 0.0000000\n",
      "Epoch: 12\tLoss: 0.0000000\n",
      "Epoch: 13\tLoss: 0.0000000\n",
      "Epoch: 14\tLoss: 0.0000000\n",
      "Epoch: 15\tLoss: 0.0000000\n",
      "Epoch: 16\tLoss: 0.0000000\n",
      "Epoch: 17\tLoss: 0.0000000\n",
      "Epoch: 18\tLoss: 0.0000000\n",
      "Epoch: 19\tLoss: 0.0000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# XOR dataset\n",
    "\n",
    "X = np.reshape([[0, 0], [0, 1], [1, 0], [1, 1]], (4, 2, 1))\n",
    "# print (\"X: \", X)\n",
    "Y = np.reshape([[0], [1], [1], [0]], (4, 1, 1))\n",
    "\n",
    "# MNIST dataset\n",
    "\n",
    "# X = np.array([np.array(x[0]).flatten() for x in train_dataset])\n",
    "# Y = np.array([x[1] for x in train_dataset])\n",
    "\n",
    "# X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "# Y = np.reshape(Y, (Y.shape[0], 1, 1))\n",
    "\n",
    "# print (\"X: \", X)\n",
    "# print (\"X shape:\", X.shape)\n",
    "# print (\"Y: \", Y)\n",
    "# print (\"Y shape:\", Y.shape)\n",
    "\n",
    "network = [\n",
    "    DenseLayer(\"Dense1\", X.shape[1], 3),\n",
    "    ReLUActivationLayer(\"ReLU1\"),\n",
    "    DenseLayer(\"Dense2\", 3, 1),\n",
    "    ReLUActivationLayer(\"ReLU2\")\n",
    "]\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    for x, y in zip(X, Y):\n",
    "        output = x\n",
    "        # print (\"output: \", output)\n",
    "        for layer in network:\n",
    "            # print (\"layer: \", layer.name)\n",
    "            output = layer.forward(output)\n",
    "\n",
    "        # loss += mse_loss(y, output)\n",
    "        # output_grad = mse_loss_prime(y, output)\n",
    "\n",
    "        loss += cross_entropy_loss(y, output)\n",
    "        output_grad = cross_entropy_loss_prime(y, output)\n",
    "\n",
    "        for layer in reversed(network):\n",
    "            output_grad = layer.backward(output_grad, learning_rate)\n",
    "\n",
    "    # if epoch % 1000 == 0:\n",
    "   \n",
    "    print(f'Epoch: {epoch}\\tLoss: {loss:.7f}')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
