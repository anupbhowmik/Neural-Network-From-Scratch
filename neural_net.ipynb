{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 66,
     "status": "ok",
     "timestamp": 1704254935274,
     "user": {
      "displayName": "Anup Bhowmik",
      "userId": "15509527806084651720"
     },
     "user_tz": -360
    },
    "id": "c5yenXYuIcTl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.datasets as ds\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 66,
     "status": "ok",
     "timestamp": 1704254935276,
     "user": {
      "displayName": "Anup Bhowmik",
      "userId": "15509527806084651720"
     },
     "user_tz": -360
    },
    "id": "8IkT4pc-IcTu"
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    The base layer class for all layers\n",
    "\n",
    "    Attributes:\n",
    "        name: The name of the layer\n",
    "        input_shape: The shape of the input to the layer\n",
    "        output_shape: The shape of the output of the layer\n",
    "\n",
    "    Methods:\n",
    "        forward: The forward pass of the layer\n",
    "        backward: The backward pass of the layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name):\n",
    "        \"\"\"\n",
    "        The constructor for the layer class\n",
    "\n",
    "        Parameters:\n",
    "            name: The name of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        self.name = name\n",
    "        self.input_shape = None\n",
    "        self.output_shape = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        The forward pass of the layer\n",
    "\n",
    "        Parameters:\n",
    "            x: The input to the layer\n",
    "\n",
    "        Returns:\n",
    "            The output of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, output_grad):\n",
    "        # we can use an optimizer instead of learning rate\n",
    "        \"\"\"\n",
    "        The backward pass of the layer\n",
    "\n",
    "        Parameters:\n",
    "            grad: The gradient of the loss w.r.t. the output of the layer\n",
    "\n",
    "        Returns:\n",
    "            The gradient of the loss w.r.t. the input of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 64,
     "status": "ok",
     "timestamp": 1704254935276,
     "user": {
      "displayName": "Anup Bhowmik",
      "userId": "15509527806084651720"
     },
     "user_tz": -360
    },
    "id": "avpQdxEbIcTx"
   },
   "outputs": [],
   "source": [
    "def xavier_initialization(m, n):\n",
    "    \"\"\"\n",
    "    Xavier initialization for the weights of a layer\n",
    "\n",
    "    Parameters:\n",
    "        m: The number of rows in the weight matrix\n",
    "        n: The number of columns in the weight matrix\n",
    "\n",
    "    Returns:\n",
    "        The initialized weights\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(82)\n",
    "\n",
    "    std_dev = np.sqrt(2.0 / (m + n))\n",
    "    return np.random.normal(0, std_dev, size=(m, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 63,
     "status": "ok",
     "timestamp": 1704254935277,
     "user": {
      "displayName": "Anup Bhowmik",
      "userId": "15509527806084651720"
     },
     "user_tz": -360
    },
    "id": "vqhi3b3KIcTz"
   },
   "outputs": [],
   "source": [
    "class DenseLayer(Layer):\n",
    "    \"\"\"\n",
    "    The dense layer class\n",
    "\n",
    "    Attributes:\n",
    "        name: The name of the layer\n",
    "        input_shape: The shape of the input to the layer\n",
    "        output_shape: The shape of the output of the layer\n",
    "        weights: The weights of the layer\n",
    "        bias: The bias of the layer\n",
    "\n",
    "    Methods:\n",
    "        forward: The forward pass of the layer\n",
    "        backward: The backward pass of the layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, input_shape, output_shape):\n",
    "        \"\"\"\n",
    "        The constructor for the dense layer class\n",
    "\n",
    "        Parameters:\n",
    "            name: The name of the layer\n",
    "            input_shape: The shape of the input to the layer\n",
    "            output_shape: The shape of the output of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(name)\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "        # for adam optimizer\n",
    "        self.m_wt = None\n",
    "        self.v_wt = None\n",
    "\n",
    "        self.m_b = None\n",
    "        self.v_b = None\n",
    "\n",
    "        self.t = 0\n",
    "\n",
    "        self.weights = xavier_initialization(self.output_shape, self.input_shape)\n",
    "        self.bias = np.zeros((self.output_shape, 1))\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        The forward pass of the layer\n",
    "\n",
    "        Parameters:\n",
    "            input: The input to the layer\n",
    "\n",
    "        Returns:\n",
    "            The output of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        self.input = input\n",
    "\n",
    "        # print (\"input shape: \", self.input.shape)\n",
    "        # print (\"weights shape: \", self.weights.shape)\n",
    "        # print (\"bias shape: \", self.bias.shape)\n",
    "\n",
    "        return np.dot(self.weights, self.input) + self.bias\n",
    "\n",
    "\n",
    "    def backward(self, output_grad):\n",
    "        \"\"\"\n",
    "        The backward pass of the layer\n",
    "\n",
    "        Parameters:\n",
    "            output_grad: The gradient of the loss w.r.t. the output of the layer\n",
    "\n",
    "        Returns:\n",
    "            The gradient of the loss w.r.t. the input of the layer\n",
    "        \"\"\"\n",
    "        # output_grad: dL/dy (L: loss, y: output of the layer)\n",
    "        # output_grad: The gradient of the loss w.r.t. the output of the layer\n",
    "        num_cols = self.input.shape[1]\n",
    "        self.weights_grad = np.dot(output_grad, self.input.T)/num_cols\n",
    "        # dL/dB = dL/dy * dy/dB = dL/dy * 1\n",
    "        self.bias_grad = np.sum(output_grad, axis=1, keepdims=True)/num_cols\n",
    "\n",
    "        # input_grad: dL/dx\n",
    "        input_grad = np.dot(self.weights.T, output_grad) / num_cols\n",
    "\n",
    "        return input_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 63,
     "status": "ok",
     "timestamp": 1704254935278,
     "user": {
      "displayName": "Anup Bhowmik",
      "userId": "15509527806084651720"
     },
     "user_tz": -360
    },
    "id": "-geiDFzdIcT1"
   },
   "outputs": [],
   "source": [
    "class ActivationLayer(Layer):\n",
    "    \"\"\"\n",
    "    The activation layer class\n",
    "\n",
    "    Attributes:\n",
    "        name: The name of the layer\n",
    "        input_shape: The shape of the input to the layer\n",
    "        output_shape: The shape of the output of the layer\n",
    "\n",
    "    Methods:\n",
    "        forward: The forward pass of the layer\n",
    "        backward: The backward pass of the layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, activation_func, activation_func_prime):\n",
    "        \"\"\"\n",
    "        The constructor for the activation layer class\n",
    "\n",
    "        Parameters:\n",
    "            name: The name of the layer\n",
    "            activation_func: The activation function of the layer\n",
    "            activation_func_prime: The derivative of the activation function of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(name)\n",
    "\n",
    "\n",
    "        self.activation_func = activation_func\n",
    "        self.activation_func_prime = activation_func_prime\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        The forward pass of the layer\n",
    "\n",
    "        Parameters:\n",
    "            input: The input to the layer\n",
    "\n",
    "        Returns:\n",
    "            The output of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        self.input = input\n",
    "\n",
    "        return self.activation_func(input)\n",
    "\n",
    "    def backward(self, output_grad):\n",
    "        \"\"\"\n",
    "        The backward pass of the layer\n",
    "\n",
    "        Parameters:\n",
    "            output_grad: The gradient of the loss w.r.t. the output of the layer\n",
    "            learning_rate: dummy for activation layer\n",
    "\n",
    "        Returns:\n",
    "            The gradient of the loss w.r.t. the input of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        return np.multiply(output_grad, self.activation_func_prime(self.input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1704254935279,
     "user": {
      "displayName": "Anup Bhowmik",
      "userId": "15509527806084651720"
     },
     "user_tz": -360
    },
    "id": "LAaviaXKIcT3"
   },
   "outputs": [],
   "source": [
    "class SoftmaxLayer(Layer):\n",
    "    \"\"\"\n",
    "    The softmax layer class\n",
    "\n",
    "    Attributes:\n",
    "        name: The name of the layer\n",
    "        input_shape: The shape of the input to the layer\n",
    "        output_shape: The shape of the output of the layer\n",
    "\n",
    "    Methods:\n",
    "        forward: The forward pass of the layer\n",
    "        backward: The backward pass of the layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name):\n",
    "        \"\"\"\n",
    "        The constructor for the softmax layer class\n",
    "\n",
    "        Parameters:\n",
    "            name: The name of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(name)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        The forward pass of the layer\n",
    "\n",
    "        Parameters:\n",
    "            input: The input to the layer\n",
    "\n",
    "        Returns:\n",
    "            The output of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        input -= np.max(input, axis=0, keepdims=True)\n",
    "\n",
    "        # Compute the numerator and denominator\n",
    "        numerator = np.exp(input)\n",
    "        denominator = np.sum(numerator, axis=0, keepdims=True)\n",
    "\n",
    "        # Compute the softmax probabilities\n",
    "        self.output = numerator / denominator\n",
    "\n",
    "        return self.output\n",
    "\n",
    "\n",
    "    def backward(self, output_grad, y_label):\n",
    "        \"\"\"\n",
    "        The backward pass of the layer\n",
    "\n",
    "        Returns:\n",
    "            The gradient of the loss w.r.t. the input of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        return self.output - y_label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 61,
     "status": "ok",
     "timestamp": 1704254935280,
     "user": {
      "displayName": "Anup Bhowmik",
      "userId": "15509527806084651720"
     },
     "user_tz": -360
    },
    "id": "2L1AncZ3IcT5"
   },
   "outputs": [],
   "source": [
    "class DropoutLayer(Layer):\n",
    "    \"\"\"\n",
    "    The dropout layer class\n",
    "\n",
    "    Attributes:\n",
    "        name: The name of the layer\n",
    "        input_shape: The shape of the input to the layer\n",
    "        output_shape: The shape of the output of the layer\n",
    "        dropout_rate: The dropout rate of the layer\n",
    "\n",
    "    Methods:\n",
    "        forward: The forward pass of the layer\n",
    "        backward: The backward pass of the layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, dropout_rate):\n",
    "        \"\"\"\n",
    "        The constructor for the dropout layer class\n",
    "\n",
    "        Parameters:\n",
    "            name: The name of the layer\n",
    "            dropout_rate: The dropout rate of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(name)\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.dropout_mask = None\n",
    "\n",
    "\n",
    "    def forward(self, input, training = True):\n",
    "        \"\"\"\n",
    "        The forward pass of the layer\n",
    "\n",
    "        Parameters:\n",
    "            input: The input to the layer\n",
    "\n",
    "        Returns:\n",
    "            The output of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        if not training:\n",
    "            return input\n",
    "\n",
    "        self.input = input\n",
    "\n",
    "        np.random.seed(82)\n",
    "        D1 = np.random.rand(*input.shape) < (1 - self.dropout_rate)\n",
    "        # D1 is a binary mask\n",
    "        # print (\"D1: \\n\", D1)\n",
    "\n",
    "        self.dropout_mask = D1 / (1 - self.dropout_rate)\n",
    "\n",
    "        # print (\"dropout mask: \\n\", self.dropout_mask)\n",
    "\n",
    "        return np.multiply(input, self.dropout_mask) / (1 - self.dropout_rate)\n",
    "        # h/(1-p)\n",
    "\n",
    "    def backward(self, output_grad):\n",
    "        \"\"\"\n",
    "        The backward pass of the layer\n",
    "\n",
    "        Parameters:\n",
    "            output_grad: The gradient of the loss w.r.t. the output of the layer\n",
    "            learning_rate: dummy for dropout layer\n",
    "\n",
    "        Returns:\n",
    "            The gradient of the loss w.r.t. the input of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        return np.multiply(output_grad, self.dropout_mask) / (1 - self.dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 60,
     "status": "ok",
     "timestamp": 1704254935280,
     "user": {
      "displayName": "Anup Bhowmik",
      "userId": "15509527806084651720"
     },
     "user_tz": -360
    },
    "id": "ExLfx9tIIcT7"
   },
   "outputs": [],
   "source": [
    "class ReLUActivationLayer(ActivationLayer):\n",
    "    \"\"\"\n",
    "    The ReLU activation layer class\n",
    "\n",
    "    Attributes:\n",
    "        name: The name of the layer\n",
    "        input_shape: The shape of the input to the layer\n",
    "        output_shape: The shape of the output of the layer\n",
    "\n",
    "    Methods:\n",
    "        forward: The forward pass of the layer\n",
    "        backward: The backward pass of the layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name):\n",
    "        \"\"\"\n",
    "        The constructor for the ReLU activation layer class\n",
    "\n",
    "        Parameters:\n",
    "            name: The name of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        # super().__init__(name, lambda x: np.maximum(x, 0), lambda x: np.where(x > 0, 1, 0))\n",
    "        super().__init__(name, relu, relu_derivative)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvL7pvxXIcT9"
   },
   "source": [
    "Cross Entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 59,
     "status": "ok",
     "timestamp": 1704254935281,
     "user": {
      "displayName": "Anup Bhowmik",
      "userId": "15509527806084651720"
     },
     "user_tz": -360
    },
    "id": "9TM_GEMuIcUB"
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y, y_pred):\n",
    "    \"\"\"\n",
    "    The cross entropy loss function\n",
    "\n",
    "    Parameters:\n",
    "        y: The ground truth\n",
    "        y_pred: The predictions\n",
    "\n",
    "    Returns:\n",
    "        The loss\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    epsilon = 1e-15  # small value to avoid log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    # limits the values in the y_pred array to be within the range\n",
    "    # [epsilon, 1 - epsilon]. This ensures that the predicted probabilities are not exactly zero or one\n",
    "\n",
    "    # print (\"in cross entropy loss\")\n",
    "    # print (\"y size: \", y.size)\n",
    "    # print (\"y_pred: \", y_pred)\n",
    "\n",
    "    return -np.sum(np.multiply(y, np.log(y_pred))) / y.size\n",
    "\n",
    "\n",
    "\n",
    "def cross_entropy_loss_prime(y, y_pred):\n",
    "    \"\"\"\n",
    "    The derivative of the cross entropy loss function with respect to the y_pred\n",
    "\n",
    "    Parameters:\n",
    "        y: The ground truth\n",
    "        y_pred: The predictions\n",
    "\n",
    "    Returns:\n",
    "        The derivative of the loss w.r.t. the predictions\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "\n",
    "    # print (\"in cross entropy loss prime\")\n",
    "    # print (\"y size: \", y.size)\n",
    "    # print (\"y_pred: \", y_pred)\n",
    "\n",
    "    return -np.divide(y, y_pred) / y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXC2ckXdIcUC"
   },
   "source": [
    "Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 59,
     "status": "ok",
     "timestamp": 1704254935282,
     "user": {
      "displayName": "Anup Bhowmik",
      "userId": "15509527806084651720"
     },
     "user_tz": -360
    },
    "id": "aiIybqzHIcUD"
   },
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    \"\"\"\n",
    "    The Adam optimizer class\n",
    "\n",
    "    Attributes:\n",
    "        learning_rate: The learning rate\n",
    "        beta1: The beta1 parameter\n",
    "        beta2: The beta2 parameter\n",
    "        epsilon: The epsilon parameter\n",
    "\n",
    "    Methods:\n",
    "        update: Updates the parameters\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate=5e-3, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Updates the parameters\n",
    "\n",
    "        Parameters:\n",
    "            layer: The layer to update\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        dw = layer.weights_grad\n",
    "        db = layer.bias_grad\n",
    "\n",
    "        if layer.m_wt is None:\n",
    "           \n",
    "            layer.m_wt = np.zeros_like(dw)\n",
    "            layer.v_wt = np.zeros_like(dw)\n",
    "\n",
    "            layer.m_b = np.zeros_like(db)\n",
    "            layer.v_b = np.zeros_like(db)\n",
    "\n",
    "        layer.t += 1\n",
    "\n",
    "        layer.m_wt = self.beta1 * layer.m_wt + (1 - self.beta1) * dw\n",
    "        layer.v_wt = self.beta2 * layer.v_wt + (1 - self.beta2) * (dw ** 2)\n",
    "\n",
    "        m_hat_wt = layer.m_wt / (1 - self.beta1 ** layer.t)\n",
    "        v_hat_wt = layer.v_wt / (1 - self.beta2 ** layer.t)\n",
    "\n",
    "        layer.weights -= self.learning_rate * m_hat_wt / (np.sqrt(v_hat_wt) + self.epsilon)\n",
    "\n",
    "\n",
    "        layer.m_b = self.beta1 * layer.m_b + (1 - self.beta1) * db\n",
    "        layer.v_b = self.beta2 * layer.v_b + (1 - self.beta2) * (db ** 2)\n",
    "\n",
    "        m_hat_b = layer.m_b / (1 - self.beta1 ** layer.t)\n",
    "        v_hat_b = layer.v_b / (1 - self.beta2 ** layer.t)\n",
    "\n",
    "        layer.bias -= self.learning_rate * m_hat_b / (np.sqrt(v_hat_b) + self.epsilon)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoRzOthoIcUE"
   },
   "source": [
    "Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58712,
     "status": "ok",
     "timestamp": 1704254993936,
     "user": {
      "displayName": "Anup Bhowmik",
      "userId": "15509527806084651720"
     },
     "user_tz": -360
    },
    "id": "S6Uqm-IeIcUF",
    "outputId": "ac0caf6c-9908-49f3-c5a6-fbb5a5f30641"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_validation_dataset = ds.EMNIST(root='./data', split='letters',\n",
    "                              train=True,\n",
    "                              transform=transforms.ToTensor(),\n",
    "                              download=True)\n",
    "\n",
    "\n",
    "independent_test_dataset = ds.EMNIST(root='./data', split='letters',\n",
    "                             train=False,\n",
    "                             transform=transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 18740,
     "status": "ok",
     "timestamp": 1704255012616,
     "user": {
      "displayName": "Anup Bhowmik",
      "userId": "15509527806084651720"
     },
     "user_tz": -360
    },
    "id": "b5fuWJ6tIcUG"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_dataset, validation_dataset = train_test_split(train_validation_dataset, test_size=0.15, random_state=82)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FZgSNVWIcUG"
   },
   "source": [
    "Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1704255012618,
     "user": {
      "displayName": "Anup Bhowmik",
      "userId": "15509527806084651720"
     },
     "user_tz": -360
    },
    "id": "2mERFjn0IcUH"
   },
   "outputs": [],
   "source": [
    "def show_images(dataset, num_images=6):\n",
    "    \"\"\"\n",
    "    Show the first num_images images of the dataset\n",
    "\n",
    "    Parameters:\n",
    "        dataset: The dataset to show the images from\n",
    "        num_images: The number of images to show\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a figure to display the images\n",
    "    fig = plt.figure()\n",
    "\n",
    "    # Loop over the first num_images images in the dataset\n",
    "    for i in range(num_images):\n",
    "        # Get the image and its label\n",
    "        image, label = dataset[i]\n",
    "\n",
    "        # The image needs to be transposed to be displayed correctly\n",
    "        image = image.transpose(0,2).transpose(0,1)\n",
    "\n",
    "        # Display the image\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow(image.squeeze(), cmap='gray')\n",
    "        plt.title(f'Label: {label}')\n",
    "        plt.axis('off')\n",
    "\n",
    "    # Display the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1704255012619,
     "user": {
      "displayName": "Anup Bhowmik",
      "userId": "15509527806084651720"
     },
     "user_tz": -360
    },
    "id": "YTahGVYJIcUH",
    "outputId": "55ff6aa7-6d2a-415c-b576-3fdcacb27ab9"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_images(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7B6HnzhkIcUJ"
   },
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1704255012620,
     "user": {
      "displayName": "Anup Bhowmik",
      "userId": "15509527806084651720"
     },
     "user_tz": -360
    },
    "id": "yLl6WnePIcUK"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(x, y):\n",
    "\n",
    "    # normalize x\n",
    "    scaler = MinMaxScaler()\n",
    "    x = scaler.fit_transform(x.reshape(x.shape[0], 28 * 28))\n",
    "    x = x.reshape(x.shape[0], 28 * 28)\n",
    "\n",
    "    # reduce each value in y by 1, so that the range of y is [0, 25]\n",
    "    y = y - 1\n",
    "\n",
    "    # convert class labels to one-hot encoded, should have shape (?, 26, 1)\n",
    "    # 26 classes: 26 letters, for example, C is 3rd letter, then C is represented as [0, 0, 1, 0, 0, ...]\n",
    "    y = np.eye(26)[y]\n",
    "\n",
    "    y = y.reshape(y.shape[0], 26)\n",
    "\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "executionInfo": {
     "elapsed": 8478,
     "status": "ok",
     "timestamp": 1704255021084,
     "user": {
      "displayName": "Anup Bhowmik",
      "userId": "15509527806084651720"
     },
     "user_tz": -360
    },
    "id": "KVkdndHVIcUK"
   },
   "outputs": [],
   "source": [
    "X = np.array([np.array(x[0]).flatten() for x in train_dataset])\n",
    "Y = np.array([x[1] for x in train_dataset])\n",
    "\n",
    "X_validation = np.array([np.array(x[0]).flatten() for x in validation_dataset])\n",
    "Y_validation = np.array([x[1] for x in validation_dataset])\n",
    "\n",
    "X_test = np.array([np.array(x[0]).flatten() for x in independent_test_dataset])\n",
    "Y_test = np.array([x[1] for x in independent_test_dataset])\n",
    "\n",
    "\n",
    "X, Y = preprocess_data(X, Y)\n",
    "X_validation, Y_validation = preprocess_data(X_validation, Y_validation)\n",
    "X_test, Y_test = preprocess_data(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A95WxLrwIcUL"
   },
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkModel:\n",
    "    \"\"\"\n",
    "    The neural network model class\n",
    "\n",
    "    Attributes:\n",
    "        layers: The layers of the model\n",
    "        loss_func: The loss function of the model\n",
    "        optimizer: The optimizer of the model\n",
    "\n",
    "    Methods:\n",
    "        forward: The forward pass of the model\n",
    "        backward: The backward pass of the model\n",
    "        update: Updates the parameters of the model\n",
    "        fit: Trains the model\n",
    "        predict: Predicts the output of the model\n",
    "        evaluate: Evaluates the model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers, loss_func, loss_func_prime, optimizer):\n",
    "        \"\"\"\n",
    "        The constructor for the neural network model class\n",
    "\n",
    "        Parameters:\n",
    "            layers: The layers of the model\n",
    "            loss_func: The loss function of the model\n",
    "            optimizer: The optimizer of the model\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        self.layers = layers\n",
    "        self.loss_func = loss_func\n",
    "        self.loss_func_prime = loss_func_prime\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def forward(self, input, training = True):\n",
    "        \"\"\"\n",
    "        The forward pass of the model\n",
    "\n",
    "        Parameters:\n",
    "            input: The input to the model\n",
    "\n",
    "        Returns:\n",
    "            The output of the model\n",
    "        \"\"\"\n",
    "\n",
    "        # print (\"in forward\")\n",
    "        # print (\"input shape: \", input.shape)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, DropoutLayer):\n",
    "                input = layer.forward(input, training=training)\n",
    "\n",
    "            else:\n",
    "              input = layer.forward(input)\n",
    "        # print (\"output shape: \", input.shape)\n",
    "\n",
    "        return input\n",
    "\n",
    "    def backward(self, output_grad, y_label):\n",
    "        \"\"\"\n",
    "        The backward pass of the model\n",
    "\n",
    "        Parameters:\n",
    "            output_grad: The gradient of the loss w.r.t. the output of the model\n",
    "            y_label: The ground truth\n",
    "\n",
    "        Returns:\n",
    "            The gradient of the loss w.r.t. the input of the model\n",
    "        \"\"\"\n",
    "\n",
    "        # print (\"in backward\")\n",
    "        # print (\"output_grad shape: \", output_grad.shape)\n",
    "\n",
    "        for layer in reversed(self.layers):\n",
    "            if isinstance(layer, SoftmaxLayer):\n",
    "                output_grad = layer.backward(output_grad, y_label)\n",
    "\n",
    "            else:\n",
    "                output_grad = layer.backward(output_grad)\n",
    "\n",
    "\n",
    "        # print (\"input_grad shape: \", output_grad.shape)\n",
    "\n",
    "        return output_grad\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Updates the parameters of the model\n",
    "        \"\"\"\n",
    "\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, DenseLayer):\n",
    "                self.optimizer.update(layer)\n",
    "\n",
    "    def fit(self, x_train, y_train, x_validation, y_validation, epochs, batch_size):\n",
    "        \"\"\"\n",
    "        Trains the model\n",
    "\n",
    "        Parameters:\n",
    "            x_train: The training input\n",
    "            y_train: The training ground truth\n",
    "            x_validation: The validation input\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # print (\"in fit\")\n",
    "        # print (\"x_train shape: \", x_train.shape)\n",
    "        # print (\"y_train shape: \", y_train.shape)\n",
    "\n",
    "        num_batches = math.ceil(x_train.shape[0] / batch_size)\n",
    "\n",
    "        self.train_losses = []\n",
    "        self.validation_losses = []\n",
    "\n",
    "        self.train_accuracy_list = []\n",
    "        self.validation_accuracy_list = []\n",
    "\n",
    "        self.train_f1_list = []\n",
    "        self.validation_f1_list = []\n",
    "\n",
    "        random.seed(82)\n",
    "        # shuffle the training data\n",
    "        zipped_data = list(zip(x_train, y_train))\n",
    "        random.shuffle(zipped_data)\n",
    "\n",
    "        # print (\"num_batches: \", num_batches)\n",
    "        # print (\"batch_size: \", batch_size)\n",
    "        # print (\"data size: \", len(zipped_data))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            train_loss = 0\n",
    "            validation_loss = 0\n",
    "            train_accuracy = 0\n",
    "            \n",
    "\n",
    "            for i in tqdm(range(num_batches)):\n",
    "                \n",
    "                batch = zipped_data[i * batch_size : (i + 1) * batch_size]\n",
    "                # unzip the minibatch\n",
    "                x_batch, y_batch = zip(*batch)\n",
    "\n",
    "\n",
    "                x_batch = np.array(x_batch).T\n",
    "                y_batch = np.array(y_batch).T\n",
    "                \n",
    "\n",
    "                # print (\"x_batch shape: \", x_batch.shape)\n",
    "                # print (\"y_batch shape: \", y_batch.shape)\n",
    "\n",
    "                # forward pass\n",
    "                y_pred = self.forward(x_batch, training=True)\n",
    "\n",
    "                # print (\"y_pred shape: \", y_pred.shape)\n",
    "\n",
    "                # compute loss\n",
    "                train_loss += self.loss_func(y_batch, y_pred)\n",
    "\n",
    "                # print (\"train_loss: \", train_loss)\n",
    "\n",
    "                # backward pass\n",
    "                output_grad = self.loss_func_prime(y_batch, y_pred)\n",
    "                self.backward(output_grad, y_batch)\n",
    "\n",
    "                # update parameters\n",
    "                self.update()\n",
    "\n",
    "            # validation\n",
    "            y_validation_pred = self.forward(x_validation.T, training = False)\n",
    "            validation_loss = self.loss_func(y_validation.T, y_validation_pred)\n",
    "\n",
    "            train_accuracy, train_f1 = self.get_accuracy_f1(x_train.T, y_train.T)\n",
    "            validation_accuracy, validation_f1 = self.get_accuracy_f1(x_validation.T, y_validation.T)\n",
    "\n",
    "            self.train_accuracy_list.append(train_accuracy)\n",
    "            self.validation_accuracy_list.append(validation_accuracy)\n",
    "\n",
    "            self.train_f1_list.append(train_f1)\n",
    "            self.validation_f1_list.append(validation_f1)\n",
    "\n",
    "            self.train_losses.append(train_loss/num_batches)\n",
    "            self.validation_losses.append(validation_loss)\n",
    "\n",
    "            # print(f'Epoch: {epoch + 1}, Train loss: {train_loss}, Validation loss: {validation_loss}')\n",
    "\n",
    "        return self.train_losses, self.validation_losses, self.train_accuracy_list, self.validation_accuracy_list, self.train_f1_list, self.validation_f1_list\n",
    "\n",
    "    def get_accuracy_f1(self, x_test, y_test):\n",
    "            \n",
    "        y_test_pred = self.forward(x_test, training = False)\n",
    "\n",
    "        y_test_pred = np.argmax(y_test_pred, axis=0)\n",
    "        y_test = np.argmax(y_test, axis=0)\n",
    "\n",
    "        return accuracy_score(y_test, y_test_pred) , f1_score(y_test, y_test_pred, average='macro')\n",
    "    \n",
    "    def predict(self, x_test, training = True):\n",
    "\n",
    "        y_test_pred = self.forward(x_test, training=training)\n",
    "\n",
    "        return y_test_pred\n",
    "\n",
    "    \n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        # test the model\n",
    "        y_test_pred = self.predict(X_test, training = False)\n",
    "        test_loss = self.loss_func(Y_test, y_test_pred)\n",
    "\n",
    "        y_test_pred = np.argmax(y_test_pred, axis=0)\n",
    "        y_test = np.argmax(Y_test, axis=0)\n",
    "\n",
    "        print (\"accuracy: \", accuracy_score(y_test, y_test_pred))\n",
    "        print (\"precision: \", precision_score(y_test, y_test_pred, average='macro'))\n",
    "        print (\"recall: \", recall_score(y_test, y_test_pred, average='macro'))\n",
    "        print (\"f1 score: \", f1_score(y_test, y_test_pred, average='macro'))\n",
    "\n",
    "        conf_mat = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "        # Create a heatmap for the confusion matrix\n",
    "        plt.figure(figsize=(14, 14))\n",
    "        sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "                    \n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def plot_graph(self, data1, data2, title, label1, label2):\n",
    "\n",
    "        plt.plot(data1, label = label1)\n",
    "        plt.plot(data2, label = label2)\n",
    "        plt.title(title)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def write_csv_report(self, model_name):\n",
    "        # write the train_losses, validation_losses, train_accuracy_list, validation_accuracy_list, train_f1, validation_f1 in separate csv files\n",
    "        import csv\n",
    "\n",
    "        # write losses in a single csv file\n",
    "        with open(f'losses_{model_name}.csv', 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"train_losses\", \"validation_losses\"])\n",
    "            for i in range(len(self.train_losses)):\n",
    "                writer.writerow([self.train_losses[i], self.validation_losses[i]])\n",
    "\n",
    "        # write accuracies in a single csv file\n",
    "        with open(f'accuracies_{model_name}.csv', 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"train_accuracy_list\", \"validation_accuracy_list\"])\n",
    "            for i in range(len(self.train_accuracy_list)):\n",
    "                writer.writerow([self.train_accuracy_list[i], self.validation_accuracy_list[i]])\n",
    "\n",
    "        # write f1 scores in a single csv file\n",
    "        with open(f'f1_scores_{model_name}.csv', 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"train_f1\", \"validation_f1\"])\n",
    "            for i in range(len(self.train_f1_list)):\n",
    "                writer.writerow([self.train_f1_list[i], self.train_f1_list[i]])\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the three different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-4\n",
    "model2 = NetworkModel(\n",
    "    layers=[\n",
    "        DenseLayer('Dense1', 28 * 28, 1024),\n",
    "        ReLUActivationLayer('ReLU1'),\n",
    "        DropoutLayer('Dropout1', 0.3),\n",
    "        DenseLayer('Dense2', 1024, 26),\n",
    "        SoftmaxLayer('Softmax')\n",
    "    ],\n",
    "    loss_func=cross_entropy_loss,\n",
    "    loss_func_prime=cross_entropy_loss_prime,\n",
    "    optimizer=AdamOptimizer(learning_rate= learning_rate)\n",
    ")\n",
    "\n",
    "train_losses, validation_losses, train_accuracy_list, validation_accuracy_list, train_f1, validation_f1 = model2.fit(X, Y, \n",
    "                                                                                                                   X_validation, Y_validation, epochs=4, batch_size=512)\n",
    "\n",
    "model2.evaluate(X_test.T, Y_test.T)\n",
    "model2.write_csv_report(f\"model2_LR_{learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-4\n",
    "model2 = NetworkModel(\n",
    "    layers=[\n",
    "        DenseLayer('Dense1', 28 * 28, 1024),\n",
    "        ReLUActivationLayer('ReLU1'),\n",
    "        DropoutLayer('Dropout1', 0.2),\n",
    "        DenseLayer('Dense2', 1024, 26),\n",
    "        SoftmaxLayer('Softmax')\n",
    "    ],\n",
    "    loss_func=cross_entropy_loss,\n",
    "    loss_func_prime=cross_entropy_loss_prime,\n",
    "    optimizer=AdamOptimizer(learning_rate= learning_rate)\n",
    ")\n",
    "\n",
    "train_losses, validation_losses, train_accuracy_list, validation_accuracy_list, train_f1, validation_f1 = model2.fit(X, Y, \n",
    "                                                                                                                   X_validation, Y_validation, epochs=4, batch_size=1024)\n",
    "\n",
    "model2.evaluate(X_test.T, Y_test.T)\n",
    "model2.write_csv_report(f\"model2_LR_{learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = NetworkModel(\n",
    "    layers=[\n",
    "        DenseLayer('Dense1', 28 * 28, 1024),\n",
    "        ReLUActivationLayer('ReLU1'),\n",
    "        DropoutLayer('Dropout1', 0.2),\n",
    "        DenseLayer('Dense2', 1024, 26),\n",
    "        SoftmaxLayer('Softmax')\n",
    "    ],\n",
    "    loss_func=cross_entropy_loss,\n",
    "    loss_func_prime=cross_entropy_loss_prime,\n",
    "    optimizer=AdamOptimizer(learning_rate= 5e-3)\n",
    ")\n",
    "\n",
    "train_losses, validation_losses, train_accuracy_list, validation_accuracy_list, train_f1, validation_f1 = model3.fit(X, Y, \n",
    "                                                                                                                   X_validation, Y_validation, epochs=4, batch_size=1024)\n",
    "model3.evaluate(X_test.T, Y_test.T)\n",
    "model3.write_csv_report(\"model3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the best model in pickle format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 92,
     "status": "ok",
     "timestamp": 1704255300227,
     "user": {
      "displayName": "Anup Bhowmik",
      "userId": "15509527806084651720"
     },
     "user_tz": -360
    },
    "id": "KweeQwpcIcUO"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "weights_layer_1, biases_layer_1 = model.layers[0].weights, model.layers[0].bias\n",
    "weights_layer_2, biases_layer_2 = model.layers[3].weights, model.layers[3].bias\n",
    "\n",
    "data_to_save = {\n",
    "    'weights_layer_1': weights_layer_1,\n",
    "    'biases_layer_1': biases_layer_1,\n",
    "    'weights_layer_2': weights_layer_2,\n",
    "    'biases_layer_2': biases_layer_2\n",
    "}\n",
    "\n",
    "with open('model.pickle', 'wb') as model_file:\n",
    "    pickle.dump(data_to_save, model_file)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
